@article{Gleckler2008,
author = {Gleckler, P. J. and Taylor, K. E. and Doutriaux, C.},
title = {Performance metrics for climate models},
journal = {Journal of Geophysical Research: Atmospheres},
volume = {113},
number = {D6},
pages = {},
keywords = {Performance metrics, climate models, model evaluation},
doi = {10.1029/2007JD008972},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2007JD008972},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2007JD008972},
abstract = {Objective measures of climate model performance are proposed and used to assess simulations of the 20th century, which are available from the Coupled Model Intercomparison Project (CMIP3) archive. The primary focus of this analysis is on the climatology of atmospheric fields. For each variable considered, the models are ranked according to a measure of relative error. Based on an average of the relative errors over all fields considered, some models appear to perform substantially better than others. Forming a single index of model performance, however, can be misleading in that it hides a more complex picture of the relative merits of different models. This is demonstrated by examining individual variables and showing that the relative ranking of models varies considerably from one variable to the next. A remarkable exception to this finding is that the so-called “mean model” consistently outperforms all other models in nearly every respect. The usefulness, limitations and robustness of the metrics defined here are evaluated 1) by examining whether the information provided by each metric is correlated in any way with the others, and 2) by determining how sensitive the metrics are to such factors as observational uncertainty, spatial scale, and the domain considered (e.g., tropics versus extra-tropics). An index that gauges the fidelity of model variability on interannual time-scales is found to be only weakly correlated with an index of the mean climate performance. This illustrates the importance of evaluating a broad spectrum of climate processes and phenomena since accurate simulation of one aspect of climate does not guarantee accurate representation of other aspects. Once a broad suite of metrics has been developed to characterize model performance it may become possible to identify optimal subsets for various applications.},
year = {2008}
}

@article{Botchkarev,
author = {Alexei Botchkarev},
title = {Performance metrics (error measures) in machine learning regression, forecasting and prognostics: Properties and typology},
journal = {ArXiv},
volume = {},
number = {},
pages = {},
keywords = {Performance metrics, model evaluation},
doi = {10.48550/arXiv.1809.03006},
url = {https://arxiv.org/abs/1809.03006},
eprint = {},
abstract = {Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. The intention of this study was to overview of a variety of performance metrics and approaches to their classification. The main goal of the study was to develop a typology that will help to improve our knowledge and understanding of metrics and facilitate their selection in machine learning regression, forecasting and prognostics. Based on the analysis of the structure of numerous performance metrics, we propose a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set.},
year = {2018}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  publisher={GitHub},
  howpublished={\url{https://github.com/fchollet/keras}},
}

@article{detlefsen2022torchmetrics,
  title={Torchmetrics-measuring reproducibility in pytorch},
  author={Detlefsen, Nicki Skafte and Borovec, Jiri and Schock, Justus and Jha, Ananya Harsh and Koker, Teddy and Di Liello, Luca and Stancl, Daniel and Quan, Changsheng and Grechkin, Maxim and Falcon, William},
  journal={Journal of Open Source Software},
  volume={7},
  number={70},
  pages={4101},
  year={2022},
  doi={10.21105/joss.041012}
}


@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}