@article{Gleckler2008,
author = {Gleckler, P. J. and Taylor, K. E. and Doutriaux, C.},
title = {Performance metrics for climate models},
journal = {Journal of Geophysical Research: Atmospheres},
volume = {113},
number = {D6},
pages = {},
keywords = {Performance metrics, climate models, model evaluation},
doi = {10.1029/2007JD008972},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2007JD008972},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2007JD008972},
abstract = {Objective measures of climate model performance are proposed and used to assess simulations of the 20th century, which are available from the Coupled Model Intercomparison Project (CMIP3) archive. The primary focus of this analysis is on the climatology of atmospheric fields. For each variable considered, the models are ranked according to a measure of relative error. Based on an average of the relative errors over all fields considered, some models appear to perform substantially better than others. Forming a single index of model performance, however, can be misleading in that it hides a more complex picture of the relative merits of different models. This is demonstrated by examining individual variables and showing that the relative ranking of models varies considerably from one variable to the next. A remarkable exception to this finding is that the so-called “mean model” consistently outperforms all other models in nearly every respect. The usefulness, limitations and robustness of the metrics defined here are evaluated 1) by examining whether the information provided by each metric is correlated in any way with the others, and 2) by determining how sensitive the metrics are to such factors as observational uncertainty, spatial scale, and the domain considered (e.g., tropics versus extra-tropics). An index that gauges the fidelity of model variability on interannual time-scales is found to be only weakly correlated with an index of the mean climate performance. This illustrates the importance of evaluating a broad spectrum of climate processes and phenomena since accurate simulation of one aspect of climate does not guarantee accurate representation of other aspects. Once a broad suite of metrics has been developed to characterize model performance it may become possible to identify optimal subsets for various applications.},
year = {2008}
}

@article{Botchkarev,
author = {Alexei Botchkarev},
title = {Performance metrics (error measures) in machine learning regression, forecasting and prognostics: Properties and typology},
journal = {Interdisciplinary Journal of Information, Knowledge, and Management},
volume = {14},
number = {},
pages = {45-76},
keywords = {Performance metrics, model evaluation},
doi = {10.28945/4184},
url = {https://arxiv.org/abs/1809.03006},
eprint = {},
abstract = {Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. The intention of this study was to overview of a variety of performance metrics and approaches to their classification. The main goal of the study was to develop a typology that will help to improve our knowledge and understanding of metrics and facilitate their selection in machine learning regression, forecasting and prognostics. Based on the analysis of the structure of numerous performance metrics, we propose a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set.},
year = {2019}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  publisher={GitHub},
  howpublished={\url{https://github.com/fchollet/keras}},
}

@article{detlefsen2022torchmetrics,
  title={Torchmetrics-measuring reproducibility in pytorch},
  author={Detlefsen, Nicki Skafte and Borovec, Jiri and Schock, Justus and Jha, Ananya Harsh and Koker, Teddy and Di Liello, Luca and Stancl, Daniel and Quan, Changsheng and Grechkin, Maxim and Falcon, William},
  journal={Journal of Open Source Software},
  volume={7},
  number={70},
  pages={4101},
  year={2022},
  doi={10.21105/joss.041012}
}


@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}


@article{Kratzert2022,
    doi = {10.21105/joss.04050},
    url = {https://doi.org/10.21105/joss.04050},
    year = {2022},
    publisher = {The Open Journal},
    volume = {7},
    number = {71},
    pages = {4050},
    author = {Frederik Kratzert and Martin Gauch and Grey Nearing and Daniel Klotz},
    title = {NeuralHydrology --- A Python library for Deep Learning research in hydrology},
    journal = {Journal of Open Source Software}
 }


 @misc{hydroeval2021,
  title={hydroeval: an evaluator for streamflow time series in {Python}},
  author={Hallouin, Thibault},
  year={2021},
  publisher={GitHub},
  doi={10.5281/zenodo.2591217}
}


 @article{wade2018hydroerr,
  title={Hydrostats: A {Python} Package for Characterizing Errors between Observed and Predicted Time Series},
  author={Roberts, Wade and Williams, Gustavious and Jackson, Elise and Nelson, James and Ames, Daniel},
  year={2021},
  publisher={MDPI},
  Journal = {Hydrology},
  volume = {5},
  doi={10.3390/hydrology5040066},
  url = {https://www.mdpi.com/2306-5338/5/4/66}
}


@inproceedings{Wang2020,
author = {Wang, Ying and Wen, Ming and Liu, Yepang and Wang, Yibo and Li, Zhenming and Wang, Chao and Yu, Hai and Cheung, Shing-Chi and Xu, Chang and Zhu, Zhiliang},
title = {Watchman: monitoring dependency conflicts for {Python} library ecosystem},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380426},
doi = {10.1145/3377811.3380426},
abstract = {The PyPI ecosystem has indexed millions of Python libraries to allow developers to automatically download and install dependencies of their projects based on the specified version constraints. Despite the convenience brought by automation, version constraints in Python projects can easily conflict, resulting in build failures. We refer to such conflicts as <u>D</u>ependency <u>C</u>onfict (DC) issues. Although DC issues are common in Python projects, developers lack tool support to gain a comprehensive knowledge for diagnosing the root causes of these issues. In this paper, we conducted an empirical study on 235 real-world DC issues. We studied the manifestation patterns and fixing strategies of these issues and found several key factors that can lead to DC issues and their regressions. Based on our findings, we designed and implemented Watchman, a technique to continuously monitor dependency conflicts for the PyPI ecosystem. In our evaluation, Watchman analyzed PyPI snapshots between 11 Jul 2019 and 16 Aug 2019, and found 117 potential DC issues. We reported these issues to the developers of the corresponding projects. So far, 63 issues have been confirmed, 38 of which have been quickly fixed by applying our suggested patches.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {125–135},
numpages = {11},
keywords = {Python, dependency conflicts, software ecosystem},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{Mukherjee2021,
author = {Mukherjee, Suchita and Almanza, Abigail and Rubio-Gonz\'{a}lez, Cindy},
title = {Fixing dependency errors for {Python} build reproducibility},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464797},
doi = {10.1145/3460319.3464797},
abstract = {Software reproducibility is important for re-usability and the cumulative progress of research. An important manifestation of unreproducible software is the changed outcome of software builds over time. While enhancing code reuse, the use of open-source dependency packages hosted on centralized repositories such as PyPI can have adverse effects on build reproducibility. Frequent updates to these packages often cause their latest versions to have breaking changes for applications using them. Large Python applications risk their historical builds becoming unreproducible due to the widespread usage of Python dependencies, and the lack of uniform practices for dependency version specification. Manually fixing dependency errors requires expensive developer time and effort, while automated approaches face challenges of parsing unstructured build logs, finding transitive dependencies, and exploring an exponential search space of dependency versions. In this paper, we investigate how open-source Python projects specify dependency versions, and how their reproducibility is impacted by dependency packages. We propose a tool PyDFix to detect and fix unreproducibility in Python builds caused by dependency errors. PyDFix is evaluated on two bug datasets BugSwarm and BugsInPy, both of which are built from real-world open-source projects. PyDFix analyzes a total of 2,702 builds, identifying 1,921 (71.1\%) of them to be unreproducible due to dependency errors. From these, PyDFix provides a complete fix for 859 (44.7\%) builds, and partial fixes for an additional 632 (32.9\%) builds.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {439–451},
numpages = {13},
keywords = {software reproducibility, dependency errors, build repair, Python},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  doi={10.48550/arXiv.1912.01703},
  url={https://doi.org/10.48550/arXiv.1912.01703}
}
